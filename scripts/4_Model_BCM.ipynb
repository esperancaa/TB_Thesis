{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCM-DTI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from SSCNN_train import test\n",
    "import pickle\n",
    "import torch\n",
    "from SSCNN_model import SSCNN_DTI\n",
    "from SSCNN_utils import load_train_val_test_set\n",
    "from SSCNN_dataset import NewDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import sys, json, torch, pickle\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from SSCNN_utils import get_one_bcm\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Step- Comparing my results with those in the literature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/ld199609/BCM-DTI.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is executed through the SSCNN_train.py script, which handles the training process and produces the history data later visualized in the notebook. Some changes were made to the SSCNN_train.py file in order to use exactly the same split that I used in the Barlow model so that I could compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 STEP- Results of training model whit my datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of PAPYRUS dataset for BCM-Dti model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_txt_from_csv(csv_path, output_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            line = f\"{row['smiles']} {row['sequence']} {int(row['label'])} {row['split']}\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "\n",
    "generate_data_txt_from_csv(\"Papyrus_Barlow.csv\", \"Papyrus.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of PAPYRUS + TB dataset for BCM-Dti model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_data_txt_from_csv(csv_path, output_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            line = f\"{row['smiles']} {row['sequence']} {int(row['label'])} {row['split']}\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "    print(f\"✓ Arquivo 'data.txt' criado em {output_path}\")\n",
    "\n",
    "\n",
    "generate_data_txt_from_csv(\"Papyrus_TB_Barlow.csv\", \"Papyrus_TB_Barlow.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of TB dataset for BCM-Dti model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_data_txt_from_csv(csv_path, output_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            line = f\"{row['smiles']} {row['sequence']} {int(row['label'])} {row['split']}\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "    print(f\"✓ Arquivo 'data.txt' criado em {output_path}\")\n",
    "\n",
    "\n",
    "generate_data_txt_from_csv(\"TB_BARLOW.csv\", \"TB.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fine tuning, adapting the pre-trained BCM-DTI model, originally trained with the Papyrus dataset, to the specific context of predicting drug-protein interactions associated with tuberculosis. The model's performance was evaluated on the tuberculosis test dataset, before and after fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 128, 256, 512]\n",
      "[462, 128, 256, 512]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = 2  \n",
    "\n",
    "args['d_channel_size'] = args['d_channel_size'][n]\n",
    "args['p_channel_size'] = args['p_channel_size'][n]\n",
    "print(args['d_channel_size'])  \n",
    "print(args['p_channel_size']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SSCNN_args()\n",
    "args['max_drug_seq'] = 44\n",
    "args['max_protein_seq'] = 462\n",
    "args['input_d_dim'] = len(words2idx_d_old) + 1\n",
    "args['input_p_dim'] = len(words2idx_p_old) + 1\n",
    "args['d_channel_size'][2][0] = args['max_drug_seq'] \n",
    "args['p_channel_size'][2][0] = args['max_protein_seq']\n",
    "args['d_channel_size'] = args['d_channel_size'][2]\n",
    "args['p_channel_size'] = args['p_channel_size'][2]\n",
    "args['dataset_name'] = \"TB\"\n",
    "\n",
    "model = SSCNN_DTI(args)\n",
    "model.load_state_dict(torch.load(\"TB.pkl\"))\n",
    "model.to(\"cuda\")\n",
    "\n",
    "with open(\"words2idx_d_papyrus.pkl\", \"rb\") as f:\n",
    "    words2idx_d = pickle.load(f)\n",
    "\n",
    "with open(\"words2idx_p_papyrus.pkl\", \"rb\") as f:\n",
    "    words2idx_p = pickle.load(f)\n",
    "\n",
    "input_path = \"TB\"\n",
    "trainSmiles, trainProtein, trainLabel, \\\n",
    "valSmiles, valProtein, valLabel, \\\n",
    "testSmiles, testProtein, testLabel, \\\n",
    "frag_set_d, frag_set_p, \\\n",
    "frag_len_d, frag_len_p, _, _ = load_train_val_test_set(\n",
    "    input_path, decompose=\"bcm\", decompose_protein=\"category\",\n",
    "    unseen_smiles=False, k=3, \n",
    ")\n",
    "\n",
    "testDataset = NewDataset(testSmiles, testProtein, testLabel, words2idx_d, words2idx_p, args['max_drug_seq'], args['max_protein_seq'])\n",
    "test_loader = DataLoader(testDataset, batch_size=args['batch_size'], shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    auc, auprc, acc, recall, precision, f1, logits, loss = test(test_loader, model)\n",
    "\n",
    "print(f\"\\nResults on dataset TB:\\n\"\n",
    "      f\"AUROC: {auc:.4f}\\n\"\n",
    "      f\"AUPRC: {auprc:.4f}\\n\"\n",
    "      f\"Prec: {precision:.4f}\\n\"\n",
    "      f\"Recall: {recall:.4f}\\n\"\n",
    "      f\"F1 Score: {f1:.4f}\\n\"\n",
    "      f\"Accc: {acc:.4f}\\n\"\n",
    "      f\"Loss: {loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuberculosis metrics on dataset papyrus + tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"metricstb.csv\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(\"words2idx_d_Papyrus_TB.pkl\",\"rb\") as f:\n",
    "    words2idx_d = pickle.load(f)\n",
    "with open(\"words2idx_p_Papyrus_TB.pkl\",\"rb\") as f:\n",
    "    words2idx_p = pickle.load(f)\n",
    "max_drug_seq = 44\n",
    "max_prot_seq = 462\n",
    "\n",
    "smiles_list = df[\"smiles\"].tolist()\n",
    "prot_list   = df[\"sequence\"].tolist()\n",
    "\n",
    "\n",
    "if \"label\" not in df.columns:\n",
    "    raise ValueError(\" 'label' not exists.\")\n",
    "labels_real = df[\"label\"]\n",
    "\n",
    "\n",
    "mask_valid = labels_real.notna()\n",
    "if not mask_valid.all():\n",
    "    df = df.loc[mask_valid].reset_index(drop=True)\n",
    "    smiles_list = df[\"smiles\"].tolist()\n",
    "    prot_list   = df[\"sequence\"].tolist()\n",
    "    labels_real = df[\"label\"].reset_index(drop=True)\n",
    "\n",
    "labels_real = labels_real.astype(int).clip(0,1).tolist()\n",
    "\n",
    "sm_frag, pr_frag, _ = get_one_bcm(smiles_list, prot_list, labels_real, decompose2=\"category\", k=3)\n",
    "\n",
    "with open(\"config_Papyrus_TB.json\") as f:\n",
    "    args_config = json.load(f)\n",
    "\n",
    "dataset_inf = NewDataset(sm_frag, pr_frag, labels_real, words2idx_d, words2idx_p, max_drug_seq, max_prot_seq)\n",
    "loader_inf  = DataLoader(dataset_inf, batch_size=64, shuffle=False)\n",
    "\n",
    "model = SSCNN_DTI(args_config)\n",
    "model.load_state_dict(torch.load(\"Papyrus_TB.pkl\", map_location=device))\n",
    "model.to(device).eval()\n",
    "\n",
    "scores = []\n",
    "with torch.no_grad():\n",
    "    for d, _, p, _ in loader_inf:\n",
    "        out = model(d.long().to(device), p.long().to(device)) \n",
    "        s = out.squeeze()\n",
    "\n",
    "        if (s.min() < 0) or (s.max() > 1):\n",
    "            s = torch.sigmoid(s)\n",
    "\n",
    "        scores.extend(s.detach().cpu().numpy().tolist())\n",
    "\n",
    "\n",
    "y_true  = np.array(labels_real, dtype=int)\n",
    "y_score = np.array(scores, dtype=float)\n",
    "\n",
    "\n",
    "y_pred = (y_score >= 0.5).astype(int)\n",
    "\n",
    "has_both_classes = (y_true.min() != y_true.max())\n",
    "\n",
    "metrics = {}\n",
    "if has_both_classes:\n",
    "    metrics[\"AUC\"]   = float(roc_auc_score(y_true, y_score))\n",
    "    metrics[\"AP\"]    = float(average_precision_score(y_true, y_score))  # PR-AUC\n",
    "else:\n",
    "    metrics[\"AUC\"] = None\n",
    "    metrics[\"AP\"]  = None\n",
    "\n",
    "metrics[\"Accuracy\"]  = float(accuracy_score(y_true, y_pred))\n",
    "metrics[\"F1\"]        = float(f1_score(y_true, y_pred, zero_division=0))\n",
    "metrics[\"Precision\"] = float(precision_score(y_true, y_pred, zero_division=0))\n",
    "metrics[\"Recall\"]    = float(recall_score(y_true, y_pred, zero_division=0))\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "metrics[\"MCC\"] = float(mcc)\n",
    "\n",
    "for k,v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v,float) else f\"{k}: {v}\")\n",
    "\n",
    "df_out = df.copy()\n",
    "df_out[\"score_bcmdti\"] = y_score\n",
    "df_out[\"pred_bcmdti\"]  = y_pred\n",
    "df_out.to_csv(\"predic_bcmdti4.csv\", index=False)\n",
    "\n",
    "with open(\"metric_bcmdti4.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
