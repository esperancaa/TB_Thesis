{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barlow Twins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score, roc_curve, precision_recall_curve, matthews_corrcoef, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sequence import uniprot2sequence, encode_sequences\n",
    "from utils.chem import *\n",
    "from utils.parallel import *\n",
    "from utils.sequence import encode_sequences\n",
    "from utils.chem import get_mols, get_fp  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from rdkit import Chem\n",
    "from xgboost import XGBClassifier\n",
    "from rdkit.Chem import AllChem\n",
    "import torch\n",
    "import esm\n",
    "from base_model import BaseModel\n",
    "from preprocessor import Preprocessor\n",
    "from barlow_twins import BarlowTwins\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of PAPYRUS dataset for Barlow Twins model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renames columns to match the input format expected by the Barlow Twins model, specifically smiles, sequence, and label, remove Nan, create a stratifying split column and truncate proteins and SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = pd.read_csv(\"Papyrus_merge.csv\")\n",
    "print(sample_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.rename(columns={\"SMILES\": \"smiles\", \"Sequence\": \"sequence\", 'pchembl_value_Mean': 'label'}, inplace=True)\n",
    "final_columns = [\"smiles\", \"sequence\", 'label']\n",
    "sample_data = sample_data[final_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(\n",
    "    sample_data,\n",
    "    test_size=0.3,\n",
    "    stratify=train_df['label'],  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df['split'] = 'train'\n",
    "val_df['split'] = 'val'\n",
    "test_df['split'] = 'test'\n",
    "\n",
    "sample_data = pd.concat([train_df, val_df, test_df]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_seqs = sample_data['sequence'].unique()\n",
    "seq_lengths = [len(seq) for seq in unique_seqs]\n",
    "\n",
    "print(\"Min:\", np.min(seq_lengths))\n",
    "print(\"Mean:\", np.mean(seq_lengths))\n",
    "print(\"Max:\", np.max(seq_lengths))\n",
    "print(\"95th percentile:\", np.percentile(seq_lengths, 95))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(seq_lengths, bins=50, kde=True, color=\"steelblue\")\n",
    "plt.axvline(np.percentile(seq_lengths, 95), color=\"red\", linestyle=\"--\", linewidth=2, \n",
    "            label=f\"95th percentile = {int(np.percentile(seq_lengths, 95))}\")\n",
    "\n",
    "plt.title(\"Distribution of protein sequence lengths (before truncation)\", fontsize=14)\n",
    "plt.xlabel(\"Sequence length (aa)\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_smiles = sample_data['smiles'].unique()\n",
    "smiles_lengths = [len(s) for s in unique_smiles]\n",
    "\n",
    "print(\"Min:\", np.min(smiles_lengths))\n",
    "print(\"Mean:\", np.mean(smiles_lengths))\n",
    "print(\"Max:\", np.max(smiles_lengths))\n",
    "print(\"95th percentile:\", np.percentile(smiles_lengths, 95))\n",
    "\n",
    "threshold = np.percentile(smiles_lengths, 95)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(smiles_lengths, bins=50, kde=True, color=\"darkgreen\")\n",
    "plt.axvline(threshold, color=\"red\", linestyle=\"--\", linewidth=2, \n",
    "            label=f\"95th percentile = {int(threshold)}\")\n",
    "\n",
    "plt.title(\"Distribution of SMILES string lengths (before padding)\", fontsize=14)\n",
    "plt.xlabel(\"SMILES length (characters)\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 1385\n",
    "\n",
    "train_df['sequence'] = sample_data['sequence'].apply(\n",
    "    lambda seq: seq[:max_len] if len(seq) > max_len else seq\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 6.5\n",
    "sample_data[\"label\"] = (sample_data[\"label\"] > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = sample_data[\"label\"].value_counts()\n",
    "print(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = \"Papyrus_Barlow.csv\"\n",
    "#sample_data.to_csv(test_dataset_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Step-Comparing my results with those in the literature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  BindingDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"BindingDB\"\n",
    "\n",
    "train_df = pd.read_csv(base_path + \"train.csv\")\n",
    "val_df   = pd.read_csv(base_path + \"val.csv\")\n",
    "test_df  = pd.read_csv(base_path + \"test.csv\")\n",
    "\n",
    "train_df[\"split\"] = \"train\"\n",
    "val_df[\"split\"] = \"val\"\n",
    "test_df[\"split\"] = \"test\"\n",
    "\n",
    "full_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "print(full_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.rename(columns={\"smile\": \"smiles\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.rename(columns={\"SMILES\": \"smiles\",\"Sequence\": \"sequence\",\"Label\": \"label\" }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_seqs = full_df['sequence'].unique()\n",
    "seq_lengths = [len(seq) for seq in unique_seqs]\n",
    "sorted_lengths = sorted(seq_lengths, reverse=True)\n",
    "print(sorted_lengths)\n",
    "print(\"medium size:\", np.mean(seq_lengths))\n",
    "print(\"min size:\", np.min(seq_lengths))\n",
    "print(\"max size:\", np.max(seq_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentil 95:\", np.percentile(seq_lengths, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 1341\n",
    "\n",
    "full_df['sequence'] = full_df['sequence'].apply(\n",
    "    lambda seq: seq[:max_len] if len(seq) > max_len else seq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = \"Binding_BARLOW.csv\"\n",
    "#full_df.to_csv(test_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/maxischuh/BarlowDTI.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is executed through the pretraining_pipeline.py script, which handles the training process and produces the history data later visualized in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and visualize the training history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = \"Binding_BARLOW.csv\"\n",
    "full_df.to_csv(test_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"stash/bin4\" \n",
    "\n",
    "with open(os.path.join(path, \"history.json\"), \"rb\") as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "if \"validation_loss\" in history:\n",
    "    plt.plot(history[\"validation_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"history train\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor = Preprocessor(\n",
    "    path=\"Binding_BARLOW.csv\",  \n",
    "    radius=2,\n",
    "    n_bits=1024,\n",
    "    aa_embedding=\"prot_t5\",\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "import pickle\n",
    "with open(\"preprocessor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(preprocessor, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BarlowTwins()\n",
    "model.load_model(\"stash/bin4\")  \n",
    "\n",
    "fps = preprocessor.fp\n",
    "aas = preprocessor.aa\n",
    "splits = preprocessor.split\n",
    "labels = preprocessor.label\n",
    "\n",
    "\n",
    "train_idx = [i for i, s in enumerate(splits) if s == \"train\"]\n",
    "fp_train = np.stack([fps[i] for i in train_idx])\n",
    "aa_train = np.stack([aas[i] for i in train_idx])\n",
    "y_train = [labels[i] for i in train_idx]\n",
    "\n",
    "test_idx = [i for i, s in enumerate(splits) if s == \"test\"]\n",
    "fp_test = np.stack([fps[i] for i in test_idx])\n",
    "aa_test = np.stack([aas[i] for i in test_idx])\n",
    "y_test = [labels[i] for i in test_idx]\n",
    "\n",
    "def zero_shot_batched(model, mol_array, aa_array, batch_size=256, device=\"cuda\"):\n",
    "    embeddings = []\n",
    "    n_samples = mol_array.shape[0]\n",
    "\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        mol_batch = mol_array[i:i+batch_size]\n",
    "        aa_batch = aa_array[i:i+batch_size]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            emb = model.zero_shot(mol_batch, aa_batch, device=device)\n",
    "            embeddings.append(emb)\n",
    "\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "X_train = zero_shot_batched(model, fp_train, aa_train, device=device)\n",
    "X_test = zero_shot_batched(model, fp_test, aa_test, device=device)\n",
    "\n",
    "clf = XGBClassifier(n_estimators=500, max_depth=5, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_test)[:, 1]\n",
    "roc = roc_auc_score(y_test, preds)\n",
    "pr = average_precision_score(y_test, preds)\n",
    "\n",
    "print(f\" ROC-AUC: {roc:.4f}\")\n",
    "print(f\" PR-AUC:  {pr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary = (preds >= 0.5).astype(int)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "acc = accuracy_score(y_test, y_pred_binary)\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "\n",
    "print(f\"📊 Accuracy:  {acc:.4f}\")\n",
    "print(f\"🎯 Precision: {precision:.4f}\")\n",
    "print(f\"🔁 Recall:    {recall:.4f}\")\n",
    "print(f\"🧮 F1 Score:  {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, preds)\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc)\n",
    "roc_display.plot()\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "prec, rec, _ = precision_recall_curve(y_test, preds)\n",
    "pr_display = PrecisionRecallDisplay(precision=prec, recall=rec, average_precision=pr)\n",
    "pr_display.plot()\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Step- Comparison between a base model with ESm + Morgan Fingerprints and the Barlow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base with simple sequence encoding Papyrus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"Papyrus_Barlow.csv\")\n",
    "df = df.dropna(subset=[\"smiles\", \"sequence\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "\n",
    "def smiles_to_ecfp(smiles, radius=2, nBits=1024):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits))\n",
    "    return np.zeros(nBits)\n",
    "\n",
    "def simple_seq_encode(seq, max_len=1500):\n",
    "    vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
    "    vec = [vocab.get(aa, 0) for aa in seq.upper()]\n",
    "    padded = vec[:max_len] + [0]*(max_len - len(vec)) if len(vec) < max_len else vec[:max_len]\n",
    "    return np.array(padded)\n",
    "\n",
    "df[\"fp\"] = df[\"smiles\"].apply(smiles_to_ecfp)\n",
    "\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval() \n",
    "\n",
    "sequences = [(\"protein1\", \"MESYHKPDQQLKDL...\"), ...]  \n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(sequences)\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "\n",
    "\n",
    "token_representations = results[\"representations\"][33]\n",
    "\n",
    "protein_embeddings = []\n",
    "for i, (_, seq) in enumerate(sequences):\n",
    "    emb = token_representations[i, 1:len(seq)+1].mean(0) \n",
    "    protein_embeddings.append(emb.numpy())\n",
    "\n",
    "df[\"seq_encoded\"] = df[\"sequence\"].apply(simple_seq_encode)\n",
    "\n",
    "X_baseline = np.stack(df[\"fp\"].values)\n",
    "X_seq = np.stack(df[\"seq_encoded\"].values)\n",
    "X_raw = np.concatenate([X_baseline, X_seq], axis=1)\n",
    "y = df[\"label\"].values\n",
    "\n",
    "train_idx = df[\"split\"] == \"train\"\n",
    "val_idx = df[\"split\"] == \"val\"\n",
    "test_idx = df[\"split\"] == \"test\"\n",
    "\n",
    "X_train_raw = X_raw[train_idx]\n",
    "X_val_raw = X_raw[val_idx]\n",
    "X_test_raw = X_raw[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_val = y[val_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_val_scaled = scaler.transform(X_val_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "def evaluate(model, X_train, X_test, y_train, y_test, name):\n",
    "    model.fit(X_train, y_train)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        probs = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        probs = model.decision_function(X_test)\n",
    "     \n",
    "        probs = (probs - probs.min()) / (probs.max() - probs.min())\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    print(f\"\\n Resultados para {name}\")\n",
    "    print(f\"AUC:       {roc_auc_score(y_test, probs):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(y_test, preds):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, preds):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(y_test, preds):.4f}\")\n",
    "    print(f\"Accuracy:  {accuracy_score(y_test, preds):.4f}\")\n",
    "    print(f\"MCC:       {matthews_corrcoef(y_test, preds):.4f}\")\n",
    "\n",
    "evaluate(\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, \"Baseline XGBoost\"\n",
    ")\n",
    "\n",
    "evaluate(\n",
    "    SGDClassifier(loss=\"hinge\", max_iter=1000, tol=1e-3),\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, \"Baseline Linear SVM (SGD)\"\n",
    ")\n",
    "\n",
    "X_train = zero_shot_batched(model, fp_train, aa_train, device=device)\n",
    "X_test = zero_shot_batched(model, fp_test, aa_test, device=device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base with simple sequence encoding Papyrus + TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Papyrus_TB_Barlow.csv\")\n",
    "df = df.dropna(subset=[\"smiles\", \"sequence\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "\n",
    "df[\"fp\"] = df[\"smiles\"].apply(smiles_to_ecfp)\n",
    "\n",
    "df[\"seq_encoded\"] = df[\"sequence\"].apply(simple_seq_encode)\n",
    "\n",
    "\n",
    "X_baseline = np.stack(df[\"fp\"].values)\n",
    "X_seq = np.stack(df[\"seq_encoded\"].values)\n",
    "X_raw = np.concatenate([X_baseline, X_seq], axis=1)\n",
    "y = df[\"label\"].values\n",
    "\n",
    "train_idx = df[\"split\"] == \"train\"\n",
    "val_idx = df[\"split\"] == \"val\"\n",
    "test_idx = df[\"split\"] == \"test\"\n",
    "\n",
    "X_train_raw = X_raw[train_idx]\n",
    "X_val_raw = X_raw[val_idx]\n",
    "X_test_raw = X_raw[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_val = y[val_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_val_scaled = scaler.transform(X_val_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "\n",
    "evaluate(\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, \"Baseline XGBoost\"\n",
    ")\n",
    "\n",
    "evaluate(\n",
    "    SGDClassifier(loss=\"hinge\", max_iter=1000, tol=1e-3),\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, \"Baseline Linear SVM (SGD)\"\n",
    ")\n",
    "\n",
    "X_train = zero_shot_batched(model, fp_train, aa_train, device=device)\n",
    "X_test = zero_shot_batched(model, fp_test, aa_test, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base with simple sequence encoding TB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"TB_BARLOW.csv\")\n",
    "df = df.dropna(subset=[\"smiles\", \"sequence\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "\n",
    "df[\"fp\"] = df[\"smiles\"].apply(smiles_to_ecfp)\n",
    "\n",
    "df[\"seq_encoded\"] = df[\"sequence\"].apply(simple_seq_encode)\n",
    "\n",
    "X_baseline = np.stack(df[\"fp\"].values)\n",
    "X_seq = np.stack(df[\"seq_encoded\"].values)\n",
    "X_raw = np.concatenate([X_baseline, X_seq], axis=1)\n",
    "y = df[\"label\"].values\n",
    "\n",
    "train_idx = df[\"split\"] == \"train\"\n",
    "val_idx = df[\"split\"] == \"val\"\n",
    "test_idx = df[\"split\"] == \"test\"\n",
    "\n",
    "X_train_raw = X_raw[train_idx]\n",
    "X_val_raw = X_raw[val_idx]\n",
    "X_test_raw = X_raw[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_val = y[val_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_val_scaled = scaler.transform(X_val_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "evaluate(\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, \"Baseline XGBoost\"\n",
    ")\n",
    "\n",
    "evaluate(\n",
    "    SGDClassifier(loss=\"hinge\", max_iter=1000, tol=1e-3),\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, \"Baseline Linear SVM (SGD)\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base with ESM TB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"TB_BARLOW.csv\")\n",
    "df = df.dropna(subset=[\"smiles\", \"sequence\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "df[\"fp\"] = df[\"smiles\"].apply(smiles_to_ecfp)\n",
    "\n",
    "\n",
    "\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.half().to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "sequences = [(\"protein\"+str(i), seq) for i, seq in enumerate(df[\"sequence\"].tolist())]\n",
    "\n",
    "protein_embeddings = []\n",
    "batch_size = 16 \n",
    "\n",
    "for i in tqdm(range(0, len(sequences), batch_size)):\n",
    "    batch_seqs = sequences[i:i+batch_size]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(batch_seqs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(\n",
    "            batch_tokens.to(device), \n",
    "            repr_layers=[6],  \n",
    "            return_contacts=False\n",
    "        )\n",
    "\n",
    "    token_representations = results[\"representations\"][6]\n",
    "\n",
    "    for j, (_, seq) in enumerate(batch_seqs):\n",
    "        emb = token_representations[j, 1:len(seq)+1].mean(0)\n",
    "        protein_embeddings.append(emb.cpu().numpy())\n",
    "\n",
    "\n",
    "df[\"esm_emb\"] = protein_embeddings\n",
    "\n",
    "\n",
    "X_ecfp = np.stack(df[\"fp\"].values)\n",
    "X_esm = np.stack(df[\"esm_emb\"].values)\n",
    "X_raw = np.concatenate([X_ecfp, X_esm], axis=1)\n",
    "y = df[\"label\"].values\n",
    "\n",
    "train_idx = df[\"split\"] == \"train\"\n",
    "test_idx = df[\"split\"] == \"test\"\n",
    "\n",
    "X_train_raw = X_raw[train_idx]\n",
    "X_test_raw = X_raw[test_idx]\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "def evaluate(model, X_train, X_test, y_train, y_test, name):\n",
    "    model.fit(X_train, y_train)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        probs = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        probs = model.decision_function(X_test)\n",
    "        probs = (probs - probs.min()) / (probs.max() - probs.min())\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    return {\n",
    "        \"Modelo\": name,\n",
    "        \"AUC\": roc_auc_score(y_test, probs),\n",
    "        \"F1 Score\": f1_score(y_test, preds),\n",
    "        \"Precision\": precision_score(y_test, preds),\n",
    "        \"Recall\": recall_score(y_test, preds),\n",
    "        \"Accuracy\": accuracy_score(y_test, preds),\n",
    "        \"MCC\": matthews_corrcoef(y_test, preds)\n",
    "    }\n",
    "\n",
    "results = []\n",
    "results_tb = []\n",
    "\n",
    "results_tb.append(evaluate(\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, \"ESM + ECFP XGBoost\"\n",
    "))\n",
    "\n",
    "results_tb.append(evaluate(\n",
    "    SGDClassifier(loss=\"hinge\", max_iter=1000, tol=1e-3),\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, \"ESM + ECFP Linear SVM\"\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base with ESM Papyrus + TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Papyrus_TB_Barlow.csv\")\n",
    "df = df.dropna(subset=[\"smiles\", \"sequence\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "\n",
    "df[\"fp\"] = df[\"smiles\"].apply(smiles_to_ecfp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.half().to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "sequences = [(\"protein\"+str(i), seq) for i, seq in enumerate(df[\"sequence\"].tolist())]\n",
    "\n",
    "protein_embeddings = []\n",
    "batch_size = 16 \n",
    "\n",
    "for i in tqdm(range(0, len(sequences), batch_size)):\n",
    "    batch_seqs = sequences[i:i+batch_size]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(batch_seqs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(\n",
    "            batch_tokens.to(device), \n",
    "            repr_layers=[6],  \n",
    "            return_contacts=False\n",
    "        )\n",
    "\n",
    "    token_representations = results[\"representations\"][6]\n",
    "\n",
    "    for j, (_, seq) in enumerate(batch_seqs):\n",
    "        emb = token_representations[j, 1:len(seq)+1].mean(0)\n",
    "        protein_embeddings.append(emb.cpu().numpy())\n",
    "\n",
    "\n",
    "df[\"esm_emb\"] = protein_embeddings\n",
    "\n",
    "\n",
    "X_ecfp = np.stack(df[\"fp\"].values)\n",
    "X_esm = np.stack(df[\"esm_emb\"].values)\n",
    "X_raw = np.concatenate([X_ecfp, X_esm], axis=1)\n",
    "y = df[\"label\"].values\n",
    "\n",
    "train_idx = df[\"split\"] == \"train\"\n",
    "test_idx = df[\"split\"] == \"test\"\n",
    "\n",
    "X_train_raw = X_raw[train_idx]\n",
    "X_test_raw = X_raw[test_idx]\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "results = []\n",
    "results_tb = []\n",
    "\n",
    "results_tb.append(evaluate(\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, \"ESM + ECFP XGBoost\"\n",
    "))\n",
    "\n",
    "results_tb.append(evaluate(\n",
    "    SGDClassifier(loss=\"hinge\", max_iter=1000, tol=1e-3),\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, \"ESM + ECFP Linear SVM\"\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base with ESM Papyrus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/resperanca/Tuberculosis_Tese/Data/Tuberculosis_Data/Papyrus_BARLOW.csv\")\n",
    "df = df.dropna(subset=[\"smiles\", \"sequence\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "df[\"fp\"] = df[\"smiles\"].apply(smiles_to_ecfp)\n",
    "\n",
    "\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.half().to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "sequences = [(\"protein\"+str(i), seq) for i, seq in enumerate(df[\"sequence\"].tolist())]\n",
    "\n",
    "protein_embeddings = []\n",
    "batch_size = 16 \n",
    "\n",
    "for i in tqdm(range(0, len(sequences), batch_size)):\n",
    "    batch_seqs = sequences[i:i+batch_size]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(batch_seqs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(\n",
    "            batch_tokens.to(device), \n",
    "            repr_layers=[6],  \n",
    "            return_contacts=False\n",
    "        )\n",
    "\n",
    "    token_representations = results[\"representations\"][6]\n",
    "\n",
    "    for j, (_, seq) in enumerate(batch_seqs):\n",
    "        emb = token_representations[j, 1:len(seq)+1].mean(0)\n",
    "        protein_embeddings.append(emb.cpu().numpy())\n",
    "\n",
    "\n",
    "df[\"esm_emb\"] = protein_embeddings\n",
    "\n",
    "\n",
    "X_ecfp = np.stack(df[\"fp\"].values)\n",
    "X_esm = np.stack(df[\"esm_emb\"].values)\n",
    "X_raw = np.concatenate([X_ecfp, X_esm], axis=1)\n",
    "y = df[\"label\"].values\n",
    "\n",
    "train_idx = df[\"split\"] == \"train\"\n",
    "test_idx = df[\"split\"] == \"test\"\n",
    "\n",
    "X_train_raw = X_raw[train_idx]\n",
    "X_test_raw = X_raw[test_idx]\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "results = []\n",
    "results_tb = []\n",
    "\n",
    "results_tb.append(evaluate(\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, \"ESM + ECFP XGBoost\"\n",
    "))\n",
    "\n",
    "results_tb.append(evaluate(\n",
    "    SGDClassifier(loss=\"hinge\", max_iter=1000, tol=1e-3),\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, \"ESM + ECFP Linear SVM\"\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Barlow with Papyrus (for other two datasets, just change the test_path), XGBoost and SVM, were obtained using the barlowdti_xxl.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_path = \"tb_test_BARLOW.csv\"\n",
    "barlow_model_path = \"Papyrus\"\n",
    "\n",
    "bt_model = BarlowTwins()\n",
    "bt_model.load_model(barlow_model_path)\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# ECFPs e embeddings \n",
    "test_mols = [Chem.MolFromSmiles(smi) for smi in test_df[\"smiles\"]]\n",
    "test_ecfp = [AllChem.GetMorganFingerprintAsBitVect(m, 2, nBits=1024) for m in test_mols]\n",
    "test_ecfp = np.array(test_ecfp)\n",
    "\n",
    "test_emb = encode_sequences(test_df[\"sequence\"].tolist(), encoder=\"prost_t5\")\n",
    "test_emb = np.array([np.array(x) for x in test_emb])\n",
    "\n",
    "# Conct embeddings\n",
    "test_vectors = bt_model.zero_shot(test_ecfp, test_emb)\n",
    "true_labels = test_df[\"label\"].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier()\n",
    "xgb_model.load_model(\"Papyrus_barlowdti_xxl_model_tb.json\")\n",
    "\n",
    "\n",
    "print(\"\\nXGBOOST-TB\")\n",
    "gbm_preds = xgb_model.predict_proba(test_vectors)[:, 1]\n",
    "gbm_binary = (gbm_preds >= 0.5).astype(int)\n",
    "\n",
    "roc = roc_auc_score(true_labels, gbm_preds)\n",
    "acc = accuracy_score(true_labels, gbm_binary)\n",
    "prec = precision_score(true_labels, gbm_binary)\n",
    "rec = recall_score(true_labels, gbm_binary)\n",
    "f1 = f1_score(true_labels, gbm_binary)\n",
    "mcc = matthews_corrcoef(true_labels, gbm_binary)\n",
    "pr_auc = average_precision_score(true_labels, gbm_preds)\n",
    "cm = confusion_matrix(true_labels, gbm_binary)\n",
    "\n",
    "print(f\"ROC-AUC: {roc:.4f}\")\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "print(f\"ACCURACY: {acc:.4f}\")\n",
    "print(f\"PRECISION: {prec:.4f}\")\n",
    "print(f\"RECALL: {rec:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"MCC: {mcc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(true_labels, gbm_preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.3f})\")\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve – XGBoost – Papyrus\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(true_labels, gbm_preds)\n",
    "ap = average_precision_score(true_labels, gbm_preds)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, label=f\"PR curve (AP = {ap:.3f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve – XGBoost – Papyrus\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true_labels, gbm_binary)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Pred 0\",\"Pred 1\"],\n",
    "            yticklabels=[\"True 0\",\"True 1\"])\n",
    "plt.title(\"Confusion Matrix – XGBoost – Papyrus\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "svm = joblib.load(\"Papyrus_barlowdti_xxl_model_svm.pkl\") \n",
    "scaler = joblib.load(\"Papyrus_barlowdti_xxl_svm_scaler.pkl\") #\n",
    "\n",
    "test_bt_scaled = scaler.transform(test_vectors)\n",
    "\n",
    "decision_scores = svm.decision_function(test_bt_scaled)\n",
    "binary_preds = (decision_scores >= 0).astype(int)\n",
    "\n",
    "roc = roc_auc_score(true_labels, decision_scores)\n",
    "acc = accuracy_score(true_labels, binary_preds)\n",
    "prec = precision_score(true_labels, binary_preds)\n",
    "rec = recall_score(true_labels, binary_preds)\n",
    "f1 = f1_score(true_labels, binary_preds)\n",
    "mcc = matthews_corrcoef(true_labels, binary_preds)\n",
    "pr_auc = average_precision_score(true_labels, decision_scores)\n",
    "cm = confusion_matrix(true_labels, binary_preds)\n",
    "\n",
    "print(f\"ROC-AUC: {roc:.4f}\")\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "print(f\"ACCURACY: {acc:.4f}\")\n",
    "print(f\"PRECISION: {prec:.4f}\")\n",
    "print(f\"RECALL: {rec:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"MCC: {mcc:.4f}\")\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "plt.title(\"Matriz de Confusão - SVM Linear\")\n",
    "plt.xlabel(\"Predito\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(true_labels, decision_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.3f})\")\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve – SVM – Papyrus\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, _ = precision_recall_curve(true_labels, decision_scores)\n",
    "ap = average_precision_score(true_labels, decision_scores)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, label=f\"PR curve (AP = {ap:.3f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve – SVM – Papyrus\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true_labels, binary_preds)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Pred 0\",\"Pred 1\"],\n",
    "            yticklabels=[\"True 0\",\"True 1\"])\n",
    "plt.title(\"Confusion Matrix – SVM – Papyrus\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
